{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEhF8PEX38IV"
      },
      "source": [
        "# HopBox Inference #\n",
        "This notebook is used to run inference on a trained FCN semantic segmentation model trained to segment Hop Cones in an image.\n",
        "\n",
        "The notebook loads the model and image data paths, color corrects the images, reads the info stored in QR code, and then runs inference on the images.\n",
        "\n",
        "The notebook outputs the segmentaion results to a user specified directory.\n",
        "\n",
        "The notebook also outputs a `.csv` file with the results of the inference run. The .csv file contains the following information:\n",
        "\n",
        "- Image Name\n",
        "- QR code informaton\n",
        "- Number of hop cones in the image\n",
        "- A few region properties of each hop cone (area, centroid, bounding box, etc.)\n",
        "- The median, standard deviation, and average RGB values of each hop cone\n",
        "\n",
        "\n",
        "Details on the Image data used, model training, color correction, and data analysis can be found in our [paper](https://....).\n",
        "\n",
        "\n",
        "**Note:** This is intended to be run on Google colab, but can be run on a local machine or elsewhere with little modification and the appropriate dependencies installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3lgPkAw38IZ"
      },
      "source": [
        "# 1. Mount Google Drive\n",
        "This section mounts the Google Drive to the notebook. This is necessary to access the data and model files stored on Google Drive. \n",
        "\n",
        "Make sure to change the path to the directory where the data and model files are stored.\n",
        "\n",
        "It also lists the files in the working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ht7oMuGEF3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "183c7105-da0d-410f-f65d-95f20754babb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/.shortcut-targets-by-id/1_Hc8bE9_tBvnI6EohgtsHKpmJp6iE0iR/HopBox_Publication/HopBox_colab\n",
            "/content/drive/.shortcut-targets-by-id/1_Hc8bE9_tBvnI6EohgtsHKpmJp6iE0iR/HopBox_Publication/HopBox_colab\n",
            "analyzed_images      Hop_images     ReadMe_files\n",
            "hopbox_functions.py  hoptrial1.pth  ref_babel_avg_srgb_d50.csv\n",
            "HopBox_Infer.ipynb   __pycache__\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "# list working directory here (this is the path to the folder with this notebook, the hopbox_functions.py file, the ref_babel_avg_srgb_d50.csv file and the folder containing the images you want to analyze):\n",
        "%cd \"/content/drive/MyDrive/HopBox_Publication/HopBox_colab/\"\n",
        "!pwd\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8377MPWNzNI4"
      },
      "source": [
        "# 2. Install Dependencies\n",
        "This section installs the dependencies needed to run the notebook including:\n",
        "- Cuda version of `PyTorch` and `TorchVision`, and `TorchAudio` : The libraries used to load the model and run inference\n",
        "- `Pyzbar` : A toolbox used to read the QR code in images\n",
        "- `libzbar0` : This is only necessary if running on linux based machine eg Colab. It is used by Pyzbar to read the QR code in images\n",
        "\n",
        "Also check the assigned GPU. This is necessary to make sure the GPU is being used to run inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNzzSmCFBinr"
      },
      "outputs": [],
      "source": [
        "# install basic dependencies\n",
        "!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!apt install libzbar0 # if linux based machine\n",
        "!pip install pyzbar\n",
        "\n",
        "# Check current/allocated cuda GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkVb-o-v38Ic"
      },
      "source": [
        "# 3. Load dependencies\n",
        "This section loads the dependencies needed to run the notebook.\n",
        "More functions are defined in the `hopbox_functions.py` file. Be sure to upload this file to the working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSy7ABEd38Id"
      },
      "outputs": [],
      "source": [
        "# Load more dependencies\n",
        "from __future__ import print_function\n",
        "from hopbox_functions import run_inference\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "sys.path.append(os.path.join(sys.path[0])) # add current directory to path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW_7sp7o38Ie"
      },
      "source": [
        "# 4. Load the Model and Data Paths, create output directory\n",
        "This section loads the model and data paths, and creates the output directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub8rBgPCEVKN"
      },
      "outputs": [],
      "source": [
        "# Put the path of the folder with your images here\n",
        "Base_folder = \"/content/drive/MyDrive/HopBox_Publication/HopBox_colab/\"\n",
        "inference_dir = Base_folder + \"Hop_images/\"\n",
        "\n",
        "filess = os.listdir(inference_dir)\n",
        "print(str(len(filess)) + \" image files found\")\n",
        "print(filess)\n",
        "\n",
        "Save_Images = 1 # 1 for saving images, and 0 for not saving images\n",
        "\n",
        "model_dir = Base_folder + \"hoptrial1.pth\"\n",
        "\n",
        "output_directory = Base_folder + \"analyzed_images/\"\n",
        "\n",
        "if Save_Images == 1:\n",
        "  mask_dir = output_directory + \"/Masks/\"\n",
        "  label_dir = output_directory + \"/Mask_labels/\"\n",
        "  corrected_dir = output_directory + \"/Corrected_imgs/\"\n",
        "\n",
        "  try:\n",
        "    os.makedirs(mask_dir)\n",
        "    os.makedirs(label_dir)\n",
        "    os.makedirs(corrected_dir)\n",
        "  except:\n",
        "    print(\"'\" + output_directory+ \"' already exists\")\n",
        "    \n",
        "directories = [inference_dir,\n",
        "               mask_dir,\n",
        "               label_dir,\n",
        "               corrected_dir,\n",
        "               model_dir,\n",
        "               output_directory\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EycH8dxF38If"
      },
      "source": [
        "# 5. Run Inference\n",
        "This section loops through the images in the data directory, reads the QR code, color corrects the image, runs inference, and saves the results to the output directory.\n",
        "\n",
        "It also saves the results described above to a `.csv` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1pgUoS4AWHf"
      },
      "outputs": [],
      "source": [
        "scale = 1; # should be based image size and network input size\n",
        "\n",
        "# Reference Data \n",
        "def load_csv_chart(f):\n",
        "    '''Input CSV's shape is (24, 3), with sRGB reference values\n",
        "    '''\n",
        "    data = np.loadtxt(f,delimiter=\",\")\n",
        "    assert data.shape ==(24, 3)\n",
        "    return data\n",
        "ref = load_csv_chart(Base_folder + 'ref_babel_avg_srgb_d50.csv') # loads reference RGB color values\n",
        "\n",
        "run_inference(ref=ref, directories=directories, filess=filess, scale=scale, Save_Images=Save_Images) # run inference on images in directory"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y2kih4XPyVCo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}